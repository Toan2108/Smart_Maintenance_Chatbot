import streamlit as st
import openai
import os
os.environ["TORCH_DISABLE_WATCHDOG"] = "1"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HF_HOME"] = "/tmp"  # n∆°i l∆∞u cache model
os.environ["HF_DATASETS_CACHE"] = "/tmp"
os.environ["HF_METRICS_CACHE"] = "/tmp"

import numpy as np
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

@st.cache_resource
def load_model():
    model = SentenceTransformer("all-MiniLM-L6-v2")
    return model

from sklearn.preprocessing import normalize
from utils import load_faiss_and_docs

# ‚úÖ Load API Key
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# ‚úÖ C·∫•u h√¨nh giao di·ªán
st.set_page_config(page_title="Smart Maintenance Chatbot", layout="wide")
st.title("ü§ñ Smart Maintenance Chatbot")
st.markdown("Nh·∫≠p c√¢u h·ªèi k·ªπ thu·∫≠t ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ t·ª´ K·ªπ s∆∞ chuy√™n m√¥n.")

# ‚úÖ Load d·ªØ li·ªáu FAISS v√† vƒÉn b·∫£n
index, docs = load_faiss_and_docs()

# ‚úÖ Load m√¥ h√¨nh embedding
from utils import download_and_extract_model

# Load m√¥ h√¨nh t·ª´ local (ƒë√£ ƒë∆∞·ª£c gi·∫£i n√©n)
model_path = download_and_extract_model()
model = SentenceTransformer(model_path)

# ‚úÖ Nh·∫≠p c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
query = st.text_input("üõ†Ô∏è Nh·∫≠p c√¢u h·ªèi k·ªπ thu·∫≠t:")

if query:
    query_embedding = model.encode([query])
    query_embedding = normalize(query_embedding, axis=1)

    D, I = index.search(query_embedding, k=3)

    contexts = []
    for idx in I[0]:
        if 0 <= idx < len(docs):
            contexts.append(docs[idx])

    if not contexts:
        st.error("Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p.")
    else:
        st.subheader("üìé T√†i li·ªáu tham chi·∫øu:")
        for i, ctx in enumerate(contexts):
            st.markdown(f"**{i+1}.** {ctx}")
        context_text = "\n\n".join(contexts)
        prompt = f"""
B·∫°n l√† m·ªôt k·ªπ s∆∞ b·∫£o tr√¨ chuy√™n nghi·ªáp. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë th√¥ng tin k·ªπ thu·∫≠t li√™n quan ƒë√£ ƒë∆∞·ª£c cung c·∫•p t·ª´ t√†i li·ªáu n·ªôi b·ªô:

{context_text}

D·ª±a tr√™n t√†i li·ªáu k·ªπ thu·∫≠t ·ªü tr√™n, h√£y ph√¢n t√≠ch v√† ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c cho c√¢u h·ªèi sau:

C√¢u h·ªèi: {query}

Y√™u c·∫ßu:
- ƒê∆∞a ra √≠t nh·∫•t **03 gi·∫£i ph√°p c·ª• th·ªÉ** ƒë·ªÉ x·ª≠ l√Ω v·∫•n ƒë·ªÅ.
- Gi·∫£i ph√°p ph·∫£i **li√™n quan ƒë·∫øn n·ªôi dung c√¢u h·ªèi v√† th√¥ng tin k·ªπ thu·∫≠t ƒë∆∞·ª£c cung c·∫•p ·ªü tr√™n**.
- Th√™m **01 gi·∫£i ph√°p mang t√≠nh ph√≤ng ng·ª´a** ƒë·ªÉ tr√°nh s·ª± c·ªë t√°i di·ªÖn.
- Th√™m **01 gi·∫£i ph√°p t·ª´ ChatGPT, l∆∞u √Ω: ch·ªâ th√™m th√¥ng tin ngo√†i t√†i li·ªáu n·∫øu th√¥ng tin ch·∫Øc ch·∫Øn**.
- C√¢u tr·∫£ l·ªùi c·∫ßn **ng·∫Øn g·ªçn, r√µ r√†ng, d·ªÖ hi·ªÉu**, vi·∫øt d∆∞·ªõi d·∫°ng **li·ªát k√™ ƒë√°nh s·ªë** (1, 2, 3) cho 3 gi·∫£i ph√°p ƒë·∫ßu ti√™n, c√°c gi·∫£i ph√°p ti·∫øp theo kh√¥ng ƒë√°nh s·ªë.

Ch·ªâ tr·∫£ l·ªùi ƒë√∫ng tr·ªçng t√¢m, kh√¥ng l·∫∑p l·∫°i c√¢u h·ªèi.
"""
        try:
            from openai import OpenAI
            client = OpenAI()

            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            answer = response.choices[0].message.content.strip()
            st.markdown("### ü§ñ K·∫øt qu·∫£ t·ª´ AI:")
            st.success(answer)
        except Exception as e:
            st.error(f"L·ªói khi g·ªçi OpenAI API: {e}")
